import streamlit as st
import json
import os
import re
import hashlib
import time
import csv
import pandas as pd
import configparser
from datetime import datetime
from typing import Dict, List, Tuple, DefaultDict
from collections import defaultdict
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from tenacity import retry, wait_exponential, stop_after_attempt
from io import BytesIO



# é¢„è®¾å€¼
PRESET_TYPES = ["åœ°è´¨å·¥ç¨‹", "åœ°è´¨ç‰¹å¾", "å‹˜æµ‹æŠ€æœ¯", "è¯•éªŒ", "åœ°è´¨èµ„æ–™", "å·¥å…·", "æ ·æœ¬"]
PRESET_PREDICATES = ["åŒ…å«", "éœ€è¦", "æ¥æº", "å½±å“", "ç”¨äº", "å¾—åˆ°", "å‚è€ƒ", "è°ƒæŸ¥", "é€‚ç”¨", "åæ˜ ","é¿å¼€"]
TYPE_MAP = {
    'PC': 'åœ°è´¨å·¥ç¨‹', 'GF': 'åœ°è´¨ç‰¹å¾', 'RH': 'å‹˜æµ‹æŠ€æœ¯',
    'TL': 'å·¥å…·', 'GI': 'åœ°è´¨èµ„æ–™', 'SP': 'æ ·æœ¬', 'EP': 'è¯•éªŒ'
}


# ------------------- TextSegmentor ç±» -------------------
class TextSegmentor:
    def __init__(self,provider_config):
        self.provider_config = provider_config['seg']
        self.llm = self._init_llm(self.provider_config['provider'])


    def _init_llm(self, provider):
        # ä»session_stateè·å–é…ç½®
        config = self.provider_config.get(provider, {})
        api_key = config.get('api_key', '')
        base_url = config.get('base_url', '')
        model_name = config.get('model', 'deepseek-chat')  # è·å–é€‰æ‹©çš„æ¨¡å‹åç§°
    

        if not api_key:
            st.error(f"è¯·æä¾›{provider}çš„APIå¯†é’¥")
            return None

        if provider == "kimi":
            base_url = base_url or "https://api.moonshot.cn/v1"
            return ChatOpenAI(
                api_key=api_key,
                base_url=base_url,
                model="moonshot-v1-32k",
                temperature=0.1
            )
        elif provider == "qwen":
            return ChatOpenAI(
                api_key=config['api_key'],
                base_url=config['base_url'],
                model=config.get('model', 'qwen-max'),
                temperature=0.3
            )

        elif provider == "doubao":
            return ChatOpenAI(
                api_key=config['api_key'],
                base_url=config['base_url'],
                model=config.get('model', 'doubao-1-5-pro-32k-250115'),
                temperature=0.5
            )
        
        else:  # deepseek
            base_url = base_url or "https://api.deepseek.com/v1"
            return ChatOpenAI(
                api_key=api_key,
                base_url=base_url,
                model=model_name,
                temperature=0.5
            )

    def semantic_segment(self, text: str) -> List[str]:
        prompt = """
            è¯·ä¸¥æ ¼æŒ‰"x.x.x"æ ¼å¼çš„æ•°å­—å°èŠ‚ç¼–å·è¿›è¡Œåˆ†æ®µï¼Œè§„åˆ™ï¼š
            1. å½“å‡ºç°"æ•°å­—.æ•°å­—.æ•°å­—"æ ¼å¼çš„ç¼–å·æ—¶ï¼ˆå¦‚4.3.1ï¼‰ï¼Œå°†è¯¥ç¼–å·ä¹‹åçš„å†…å®¹è§†ä¸ºæ–°æ®µè½
            2. æ®µè½æŒç»­åˆ°ä¸‹ä¸€ä¸ªåŒçº§ç¼–å·å‡ºç°ä¸ºæ­¢ï¼ˆå¦‚4.3.1çš„å†…å®¹æŒç»­åˆ°4.3.2ä¹‹å‰ï¼‰
            3. ä¿ç•™è‡ªç„¶æ®µè½ç»“æ„ï¼Œä¸è¦åˆå¹¶ä¸åŒå°èŠ‚å†…å®¹
            4. ä¿ç•™ç¼–å·å‰ç¼€ï¼Œè¾“å‡ºæ®µè½å†…å®¹
            
            ç‰¹æ®Šå¤„ç†è¦æ±‚ï¼š
            - æ•°å­—ç¼–å·åçš„ç©ºæ ¼/æ¢è¡Œç¬¦ç­‰æ ¼å¼å·®å¼‚ä¸å½±å“åˆ†æ®µåˆ¤æ–­
            - ä¿ç•™æ¡æ¬¾ä¸­çš„æ•°å­—ç¼–å·ï¼ˆå¦‚"1  éš§é“åº”é€‰æ‹©..."ä¸­çš„1/2/3ï¼‰

            ç¤ºä¾‹ï¼š
            åŸæ–‡ï¼š4.3.1 éš§é“ä½ç½®...åŸåˆ™ï¼š
            1 éš§é“åº”é€‰æ‹©...æœ‰åˆ©ã€‚
            2 éš§é“åº”é¿å¼€...
            å¤„ç†åï¼š
            éš§é“ä½ç½®...åŸåˆ™ï¼š
            1 éš§é“åº”é€‰æ‹©...æœ‰åˆ©ã€‚
            2 éš§é“åº”é¿å¼€...

            éœ€è¦åˆ†æ®µçš„æ–‡æœ¬ï¼š{text}
            """
        chunks = []
        for i in range(0, len(text), 3000):
            chunk = text[i:i+3000]
            response = self.llm.invoke(prompt.format(text=chunk)).content
            chunks.extend([p.strip() for p in response.split('\n\n') if p.strip()])
        return chunks

# ------------------- RelationExtractor ç±» -------------------
class RelationExtractor:
    def __init__(self,provider_config, relation_rules: str = None):
        self.segmentor = TextSegmentor(provider_config)
        self.provider_config = provider_config['extract']
        self.relations = self.parse_relation_rules(relation_rules) if relation_rules else {}
        self.api_config = {'max_workers': 6, 'retries': 3, 'delay': 0.8}
        self.validation_rules = {'predicate_whitelist': set(PRESET_PREDICATES)}
        
        # åˆå§‹åŒ–æç¤ºè¯
        self.template = """
                        ### ä»»åŠ¡ç›®æ ‡
                        ä½œä¸ºåœ°è´¨å‹˜æ¢é¢†åŸŸçš„ä¸“å®¶ï¼Œä»ä»¥ä¸‹åœ°è´¨å‹˜æ¢é¢†åŸŸæ–‡æœ¬è”åˆæå–ä¸‰å…ƒç»„ä»¥æ„å»ºçŸ¥è¯†å›¾è°±ï¼š{text}

                        ### å…·ä½“è¦æ±‚
                        1. **å®ä½“è¯†åˆ«ä¸åˆ†ç±»**ï¼šä»æ–‡æœ¬ä¸­è¯†åˆ«å®ä½“ï¼Œå¹¶æŒ‰ç…§ç»™å®šçš„å®ä½“ç±»å‹å¯¹å®ƒä»¬è¿›è¡Œåˆ†ç±»ï¼š{entity_types}
                            - å®ä½“ç±»å‹åŠç¤ºä¾‹ï¼š
                                - åœ°è´¨å·¥ç¨‹ - ç¤ºä¾‹ï¼šæ”¹å»ºé“è·¯ã€è¾¹å¡å·¥ç¨‹
                                - åœ°è´¨ç‰¹å¾ - ç¤ºä¾‹ï¼šç ‚åœŸå±‚ã€æ–­è£‚å¸¦
                                - å‹˜æµ‹æŠ€æœ¯ - ç¤ºä¾‹ï¼šåœ°è´¨é’»æ¢ã€ç‰©æ¢
                                - å·¥å…·- ç¤ºä¾‹ï¼šé’»æœºã€æµ‹é‡ä»ª
                                - åœ°è´¨èµ„æ–™ - ç¤ºä¾‹ï¼šåœ°è´¨å‹˜å¯ŸæŠ¥å‘Šã€å‰–é¢å›¾
                                - æ ·æœ¬ - ç¤ºä¾‹ï¼šå²©èŠ¯æ ·æœ¬ã€åœŸæ ·
                                - è¯•éªŒ - ç¤ºä¾‹ï¼šæ‰¿è½½åŠ›è¯•éªŒã€æ¸—é€è¯•éªŒ
                        2. **å…³ç³»æ¨æ–­**ï¼šæ ¹æ®ä¸Šä¸‹æ–‡ä»¥åŠç»“åˆåœ°è´¨é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†æ¨æ–­å®ä½“é—´çš„é€»è¾‘å…³ç³»ã€‚
                        3. **å…³ç³»ç±»å‹ç­›é€‰**ï¼šå…³ç³»ç±»å‹èŒƒå›´å¦‚ä¸‹ï¼š{predicate_list}ã€‚å¦‚æœæ¨æ–­çš„å…³ç³»ä¸åœ¨ç»™å®šçš„ç±»å‹ä¸­ï¼š
                            - è‹¥å…¶ä¸ç»™å®šå…³ç³»ç±»å‹é›†åˆä¸­çš„æŸä¸€ç§å…³ç³»æ‰€è¡¨è¿°çš„æ„æ€ä¸€è‡´ï¼Œåˆ™æ›¿æ¢ä¸ºç»™å®šå…³ç³»ç±»å‹ã€‚
                            - è‹¥å…¶ä¸ç»™å®šå…³ç³»ç±»å‹é›†åˆä¸­ä»»ä¸€å…³ç³»è¡¨è¿°çš„æ„æ€éƒ½ä¸ä¸€è‡´ï¼Œåˆ™è¿‡æ»¤è¯¥ä¸‰å…ƒç»„ã€‚
                        4. **ä¸‰å…ƒç»„å®Œæ•´æ€§**ï¼šæ¯ä¸ªä¸‰å…ƒç»„å¿…é¡»åŒ…å«å®Œæ•´è¦ç´ ã€‚

                        ### é¿å…æå–å†…å®¹
                        1. æŠ½è±¡æ¦‚å¿µï¼Œå¦‚â€œæœ¬è§„èŒƒç¬¬å››ç« â€ã€‚
                        2. æ³›æŒ‡è¡¨è¿°ï¼Œå¦‚â€œç›¸å…³è¦æ±‚â€ã€‚
                        3. ä¸å±äºåœ°è´¨å‹˜æ¢é¢†åŸŸçš„å†…å®¹ã€‚
                        4. åŒ…å«æ•°å­—ã€å•ä½ã€ç¬¦å·ç­‰ä¸å±äºåœ°è´¨å‹˜æ¢é¢†åŸŸçŸ¥è¯†çš„å†…å®¹

                        ### è¿”å›æ ¼å¼
                        è¿”å›æ ¼å¼ç¤ºä¾‹ï¼š[åœ°è´¨é’»æ¢|GF]â†’ç”¨äºâ†’[æ–­è£‚å¸¦|GF]ï¼Œä¸è¦åŒ…å«*å·ç­‰ç‰¹æ®Šç¬¦å·ï¼Œä¸è¦æå–æ•°å­—ã€æ¯”ä¾‹å°ºç­‰æ— æ„ä¹‰å†…å®¹ã€‚                       
                        """
        
        self.prompt = PromptTemplate(
            template=self.template,
            input_variables=["text"],
            partial_variables={
                "entity_types": "ã€".join(f"{v}({k})" for k, v in TYPE_MAP.items()),
                "predicate_list": "ã€".join(self.validation_rules['predicate_whitelist']) 
            }
        )
        
        # åˆå§‹åŒ–å¤§æ¨¡å‹
        self.extract_llm = self._init_llm(self.provider_config['provider'])
        
        # ç¼“å­˜
        self.entity_cache = defaultdict(set)
        self.relation_cache = defaultdict(set)

    def _init_llm(self, provider):
        # ä»session_stateè·å–é…ç½®
        config = self.provider_config.get(provider, {})
        api_key = config.get('api_key', '')
        base_url = config.get('base_url', '')
        model_name = config.get('model', 'deepseek-chat')  # è·å–é€‰æ‹©çš„æ¨¡å‹åç§°
    

        if not api_key:
            st.error(f"è¯·æä¾›{provider}çš„APIå¯†é’¥")
            return None

        if provider == "kimi":
            base_url = base_url or "https://api.moonshot.cn/v1"
            return ChatOpenAI(
                api_key=api_key,
                base_url=base_url,
                model="moonshot-v1-32k",
                temperature=0.1
            )
        elif provider == "qwen":
            return ChatOpenAI(
                api_key=config['api_key'],
                base_url=config['base_url'],
                model=config.get('model', 'qwen-max'),
                temperature=0.3
            )

        elif provider == "doubao":
            return ChatOpenAI(
                api_key=config['api_key'],
                base_url=config['base_url'],
                model=config.get('model', 'doubao-1-5-pro-32k-250115'),
                temperature=0.5
            )
        
        else:  # deepseek
            base_url = base_url or "https://api.deepseek.com/v1"
            return ChatOpenAI(
                api_key=api_key,
                base_url=base_url,
                model=model_name,
                temperature=0.5
            )
    @staticmethod
    def parse_relation_rules(content: str) -> Dict[Tuple[str, str], str]:
        reader = csv.reader(content.strip().splitlines())
        headers = next(reader)[1:]
        relations = {}
        
        for row in reader:
            if len(row) < 1: continue
            target = row[0].split()[-1]
            for idx, rel in enumerate(row[1:]):
                if idx >= len(headers): break
                source = headers[idx].split()[-1]
                if rel.strip() not in ('/', ''):
                    relations[(source, target)] = rel.strip()
        return relations

    def learn_from_docx(self, docx_content: bytes) -> str:
        """ä»DOCXæ–‡æ¡£ä¸­æå–å­¦ä¹ å†…å®¹"""
        doc = Document(BytesIO(docx_content))
        learned_text = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
        return learned_text[:5000]  # é™åˆ¶å­¦ä¹ é•¿åº¦
    
    def _preprocess_text(self, text: str) -> str:
        text = re.sub(r'\d+[)ã€.]', '', text)
        return re.sub(r'ï¼›', 'ã€‚', text)

    def _extract_segment_id(self, text: str) -> Tuple[str, str]:
        patterns = [
            r'^(\d+\.\d+\.\d+)\s+',
            r'^(\d+\.\d+)\s+',
            r'^ç¬¬(\d+æ¡)\s+',
            r'^(Article \d+)\s+'
        ]
        for pattern in patterns:
            if match := re.search(pattern, text[:100]):
                return (match.group(1), text[match.end():])
        return (f'seg_{int(time.time())}', text)

    def _validate_triples(self, context: str, triples: List[Dict]) -> List[Dict]:
        return [t for t in triples if self._check_relation(context, t)]
    
    def _validate_triple_structure(self, triple: Dict) -> bool:
        required_keys = {'subject', 'subject_type', 'predicate', 'object', 'object_type'}
        return all(key in triple and bool(triple[key]) for key in required_keys)

    @retry(wait=wait_exponential(multiplier=1.5), stop=stop_after_attempt(3))
    def _safe_api_call(self, func, *args, **kwargs):
        time.sleep(self.api_config['delay'])
        try:
            result = func(*args, **kwargs)
            if "rate limit" in str(result).lower():
                raise Exception("APIé€Ÿç‡é™åˆ¶è§¦å‘")
            return result
        except Exception as e:
            if "429" in str(e):
                time.sleep(30)
            raise

    def _process_chunk(self, chunk: str) -> List[Dict[str, str]]:
        cache_key = hashlib.md5(chunk[:200].encode()).hexdigest()
        if cache_key in self.relation_cache:
            return json.loads(self.relation_cache[cache_key])

        try:
            chunk = self._preprocess_text(chunk) if chunk else ""
            formatted_prompt = self.prompt.format(text=chunk)
            response = self.extract_llm.invoke(formatted_prompt, timeout=30).content
            
            raw_response = response if hasattr(response, 'content') else str(response)
            initial_triples = self._parse_joint_results(raw_response)
            
            if not isinstance(initial_triples, list):
                initial_triples = []
                
            processed_triples = []
            for triple in initial_triples:
                if not self._validate_triple_structure(triple):
                    continue

                processed_triples.append({
                    "subject": str(triple.get("subject", "")),
                    "subject_type": str(triple.get("subject_type", "")),
                    "predicate": str(triple.get("predicate", "")),
                    "object": str(triple.get("object", "")),
                    "object_type": str(triple.get("object_type", ""))
                })
                
            valid_triples = self._validate_triples(chunk, processed_triples)
            self.relation_cache[cache_key] = json.dumps(valid_triples)
            return valid_triples

        except Exception as e:
            error_msg = str(e)
            if "401" in error_msg and "Authentication Fails" in error_msg:
                st.error("æ®µè½å¤„ç†å¤±è´¥ï¼Œæ£€æµ‹åˆ° API Key è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®é¡µé¢çš„ API Key æ˜¯å¦æ­£ç¡®ã€‚")
                print("æ®µè½å¤„ç†å¤±è´¥ï¼Œæ£€æµ‹åˆ° API Key è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®é¡µé¢çš„ API Key æ˜¯å¦æ­£ç¡®ã€‚")
            else:
                st.error(f"æ®µè½å¤„ç†å¤±è´¥: {error_msg}")
                print(f"æ®µè½å¤„ç†å¤±è´¥: {error_msg}")
            return []

    def _parse_joint_results(self, raw: str) -> List[Dict]:
        triples = []  
        pattern = re.compile(
            r"\d*\.?\s*\[([^|\]]+)[\s|]*([A-Z]{2})\][\sâ†’|-]+([^â†’]+?)[\sâ†’|-]+\[([^|\]]+)[\s|]*([A-Z]{2})\]"
        )

        raw = re.sub(r'(\d+\.)\n', r'\1', raw)
        for line in raw.split('\n'):
            line = line.replace("ï¼š", ":").strip()
            if match := pattern.search(line):
                subj, subj_type, pred, obj, obj_type = match.groups()
                pred = pred.strip()
                if pred in self.validation_rules['predicate_whitelist']:
                    triples.append({
                        "subject": subj.strip(),
                        "subject_type": TYPE_MAP.get(subj_type.strip(), subj_type),
                        "predicate": pred,
                        "object": obj.strip(),
                        "object_type": TYPE_MAP.get(obj_type.strip(), obj_type),
                        "validated": False
                    })
        return triples

    def _check_relation(self, context: str, triple: Dict) -> bool:
        if triple['subject_type'].split()[0] == triple['object_type'].split()[0]:
            return False
        
        if len(triple['subject']) < 2 or len(triple['object']) < 2:
            return False

        if triple['predicate'] not in self.validation_rules['predicate_whitelist']:
            return False

        return True

    def extract(self, text: str, relation_rules: str = None) -> Dict:
        if relation_rules:
            self.relations = self.parse_relation_rules(relation_rules)
        
        chunks = self.segmentor.semantic_segment(text)
        results = []
        has_error = False  # æ–°å¢é”™è¯¯æ ‡è®°
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for idx, chunk in enumerate(chunks):
            status_text.text(f"å¤„ç†æ®µè½ {idx+1}/{len(chunks)}...")
            progress_bar.progress((idx + 1) / len(chunks))
            
            segment_id, cleaned_chunk = self._extract_segment_id(chunk)
            triples = self._process_chunk(cleaned_chunk)
            if not triples:  # è‹¥è¿”å›ç©ºåˆ—è¡¨ï¼Œè¯´æ˜å¯èƒ½å‡ºç°é”™è¯¯
                has_error = True  # æ ‡è®°å‡ºç°é”™è¯¯

            results.append({
                "segment_id": segment_id,
                "text_content": cleaned_chunk[:1000],
                "triples": triples
            })
            
            time.sleep(self.api_config['delay'])
        
        progress_bar.empty()
        status_text.empty()
        
        return {
            "metadata": {
                "format_version": "1.1",
                "generated_at": datetime.now().isoformat(),
                "source": "åœ°è´¨æ–‡æœ¬æå–"
            },
            "total_segments": len(chunks),
            "segments": results,
            "has_error": has_error  # è¿”å›é”™è¯¯æ ‡è®°
        }

    #æ–°æ–¹æ³•
    def update_prompts(self, new_prompts: dict):
        if "å…³ç³»æå–" in new_prompts:
            self.template = new_prompts["å…³ç³»æå–"]
            self.prompt = PromptTemplate(
                template=self.template,
                input_variables=["text"],
                partial_variables={
                    "entity_types": "ã€".join(f"{v}({k})" for k, v in TYPE_MAP.items()),
                    "predicate_list": "ã€".join(self.validation_rules['predicate_whitelist']) 
                }
            )


    def to_csv(self, data: Dict, output_dir: str = ".") -> Tuple[str, str]:
        all_triples = []
        for seg in data.get('segments', []):
            if isinstance(seg, dict) and 'triples' in seg:
                all_triples.extend([
                    {**triple, "segment_id": seg.get('segment_id', '')}
                    for triple in seg.get('triples', [])
                ])
        
        entities = {}
        for idx, triple in enumerate(all_triples, 1):
            for role in ['subject', 'object']:
                name = triple[f'{role}']
                ent_type = triple[f'{role}_type']
                if name not in entities:
                    entities[name] = {
                        'id': len(entities) + 1,
                        'name': name,
                        'entity_type': ent_type
                        
                    }

        entities_df = pd.DataFrame(entities.values())
        relations_df = pd.DataFrame([
            {
                'start_id': entities[triple['subject']]['id'],
                'end_id': entities[triple['object']]['id'],
                'type': triple['predicate']
            }
            for triple in all_triples
            if triple['subject'] in entities and triple['object'] in entities
        ])

        base_name = "geological_triples"
        entities_path = os.path.join(output_dir, f"{base_name}_entities.csv")
        relations_path = os.path.join(output_dir, f"{base_name}_relations.csv")

        entities_df.to_csv(entities_path, index=False,encoding='utf-8-sig')
        relations_df.to_csv(relations_path, index=False,encoding='utf-8-sig')
        
        return entities_path, relations_path
    
###########--------------------------------############
# ------------------- Streamlit åº”ç”¨ -------------------
##########---------------------------------############

DEEPSEEK_BASE_URL = "https://api.deepseek.com/v1"
DEEPSEEK_MODEL = "deepseek-chat"

PRESET_PROMPTS = {
    "åˆ†æ®µè§„åˆ™": TextSegmentor.semantic_segment.__doc__,
    "å…³ç³»æå–": RelationExtractor.__init__.__doc__
}
def parse_prompt_update(response: str) -> dict:
    """è§£æAIç”Ÿæˆçš„æç¤ºè¯ä¿®æ”¹ï¼Œå¼ºåŒ–ç»“æ„åŒ¹é…"""
    patterns = [
        r"## (åˆ†æ®µè§„åˆ™|å…³ç³»æå–)[\sï¼š:]*([\s\S]+?)(?=## |$)",  # å¼ºåŒ–æ¿å—åŒ¹é…
        r"ã€(åˆ†æ®µè§„åˆ™|å…³ç³»æå–)ã€‘[\sï¼š:]*([\s\S]+?)(?=ã€|$)"
    ]
    
    matches = []
    for pattern in patterns:
        matches.extend(re.findall(pattern, response, flags=re.DOTALL))
    
    return {
        key: value.strip()  # å»é™¤å¤šä½™ä¿®é¥°è¯
        for key, value in matches
        if key in ["åˆ†æ®µè§„åˆ™", "å…³ç³»æå–"]  # ä¸¥æ ¼åŒ¹é…é¢„è®¾é”®
    }


def main():
    global PRESET_PROMPTS
    
    # è®¾ç½®é¡µé¢é…ç½®ï¼ŒåŒ…å«ç»¿è‰²ä¸»é¢˜
    st.set_page_config(
        page_title="åœ°è´¨å·¥ç¨‹ä¸‰å…ƒç»„æå–ä¸æ ¡æ­£ç³»ç»Ÿ", 
        layout="wide",
        initial_sidebar_state="expanded",  # ä¾§è¾¹æ é»˜è®¤å±•å¼€
        page_icon="ğŸŒ±"  # ç»¿è‰²ä¸»é¢˜å›¾æ ‡
    )
    
    # è‡ªå®šä¹‰CSSæ ·å¼ï¼Œè®¾ç½®ç»¿è‰²ä¸»é¢˜
    st.markdown("""
        <style>
        /* ä¸»æ ‡é¢˜æ ·å¼ */
        .main-title {
            text-align: center;
            color: #2E7D32;
            font-size: 2.5rem;
            font-weight: bold;
            margin-bottom: 2rem;
            padding: 1rem;
            background: linear-gradient(90deg, #E8F5E8, #C8E6C9, #E8F5E8);
            border-radius: 10px;
            border-left: 5px solid #4CAF50;
        }
        
        /* ä¾§è¾¹æ æŒ‰é’®æ ·å¼ */
        .stButton > button {
            width: 100%;
            background-color: #4CAF50;
            color: white;
            border: none;
            border-radius: 8px;
            padding: 0.75rem;
            margin: 0.25rem 0;
            font-weight: bold;
            transition: all 0.3s;
        }
        
        .stButton > button:hover {
            background-color: #45A049;
        }
        </style>
    """, unsafe_allow_html=True)

    # å±…ä¸­æ˜¾ç¤ºä¸»æ ‡é¢˜
    st.markdown(
        '<h1 class="main-title">ğŸŒ± åœ°è´¨å·¥ç¨‹ä¸‰å…ƒç»„æå–ä¸æ ¡æ­£ç³»ç»Ÿ</h1>', 
        unsafe_allow_html=True
    )
    
    # åˆå§‹åŒ– session_stateï¼ˆä¿æŒä½ åŸæœ‰çš„åˆå§‹åŒ–ä»£ç ï¼‰
    if 'extracted_data' not in st.session_state:
        st.session_state.extracted_data = None
    if 'current_page' not in st.session_state:
        st.session_state.current_page = 1
    if 'prompt_history' not in st.session_state:
        st.session_state.prompt_history = []
    if 'provider_config' not in st.session_state:
        st.session_state.provider_config = {
            'seg':{
                'provider': 'kimi',
                'kimi': {'api_key': '', 'base_url': 'https://api.moonshot.cn/v1', 'model': 'moonshot-v1-32k'},
                'deepseek': {'api_key': '', 'base_url': 'https://api.deepseek.com/v1','model':'deepseek-chat'},
                'qwen': {'api_key': '', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1','model':'qwen-max'},
                'doubao': {'api_key': '', 'base_url': "https://ark.cn-beijing.volces.com/api/v3",'model':"doubao-seed-1-6-250615"}
            },
            'extract':{
                'provider': 'deepseek',
                'kimi':{'api_key': '', 'base_url': 'https://api.moonshot.cn/v1', 'model': 'moonshot-v1-32k'},
                'deepseek': {'api_key': '', 'base_url': 'https://api.deepseek.com/v1','model':'deepseek-chat'},
                'qwen': {'api_key': '', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1','model':'qwen-max'},
                'doubao': {'api_key': '', 'base_url': "https://ark.cn-beijing.volces.com/api/v3",'model':"doubao-seed-1-6-250615"}
            }
        }
    if 'correction_records' not in st.session_state:
        st.session_state.correction_records = []
    if 'original_triple_count' not in st.session_state:
        st.session_state.original_triple_count = 0

    # ä¾§è¾¹æ å¯¼èˆª
    with st.sidebar:
        st.markdown("### ğŸ“‹ åŠŸèƒ½å¯¼èˆª")
        
        # åˆå§‹åŒ–å½“å‰é¡µé¢çŠ¶æ€
        if 'current_tab' not in st.session_state:
            st.session_state.current_tab = 'config'
        
        # å¯¼èˆªæŒ‰é’®
        if st.button('âš™ï¸ é…ç½®', key='nav_config', use_container_width=True):
            st.session_state.current_tab = 'config'
            st.rerun()
            
        if st.button('ğŸ“ æ–‡æœ¬æå–', key='nav_extract', use_container_width=True):
            st.session_state.current_tab = 'extract'
            st.rerun()
            
        if st.button('âœï¸ ä¸‰å…ƒç»„æ ¡æ­£', key='nav_correct', use_container_width=True):
            st.session_state.current_tab = 'correct'
            st.rerun()
            
        if st.button('ğŸ’¾ å¯¼å‡ºç»“æœ', key='nav_export', use_container_width=True):
            st.session_state.current_tab = 'export'
            st.rerun()
            
        if st.button('ğŸ¤– AIåŠ©æ‰‹', key='nav_assistant', use_container_width=True):
            st.session_state.current_tab = 'assistant'
            st.rerun()
        
        st.markdown("---")
        
        # ä¾§è¾¹æ çŠ¶æ€ä¿¡æ¯
        if st.session_state.extracted_data:
            st.success("âœ… æ•°æ®å·²åŠ è½½")
            total_triples = sum(len(seg["triples"]) for seg in st.session_state.extracted_data["segments"])
            st.metric("ä¸‰å…ƒç»„æ€»æ•°", total_triples)
        else:
            st.info("â„¹ï¸ æš‚æ— æ•°æ®")

    # ä¸»å†…å®¹åŒºåŸŸ - ç›´æ¥ç”¨æ¡ä»¶åˆ¤æ–­ï¼Œä¸éœ€è¦é¢å¤–å‡½æ•°
    if st.session_state.current_tab == 'config':
        st.header("APIé…ç½®")
        st.info("è¯·åœ¨æ­¤é…ç½®ç”¨äºåˆ†å¥å’Œå…³ç³»æå–çš„APIæœåŠ¡")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("åˆ†å¥æœåŠ¡")
            st.session_state.provider_config['seg']['provider'] = st.selectbox(
                "åˆ†æ®µæœåŠ¡å•†",
                options=["kimi", "qwen", "doubao","deepseek"],
                index=["kimi", "qwen", "doubao","deepseek"].index(
                    st.session_state.provider_config['seg']['provider']),
                    key = "seg_provider_select"
            )
            provider = st.session_state.provider_config['seg']['provider']
        

            model_options = {
                "kimi": ["moonshot-v1-128k", "moonshot-v1-32k"],
                "qwen": ["qwen-max", "qwen-plus", "qwen-turbo"],
                "doubao": ["doubao-seed-1-6-250615", "doubao-1-5-pro-32k-250115",'doubao-1-5-thinking-pro-250415'],
                "deepseek": ["deepseek-chat", "deepseek-reasoner"]
            }

            current_models = model_options.get(provider, ["default-model"])

            st.session_state.provider_config['seg'][provider]['model'] = st.selectbox(
                f"{provider.upper()} æ¨¡å‹",
                options=current_models,
                index=current_models.index(
                    st.session_state.provider_config['seg'][provider].get('model', current_models[0])
                ),
                key=f"seg_{provider}_model"
            )
            st.session_state.provider_config['seg'][provider]['api_key'] = st.text_input(
                f"{provider.upper()} APIå¯†é’¥",
                value=st.session_state.provider_config['seg'][provider]['api_key'],
                type="password",
                key=f"seg_{provider}_api_key"
            )
            st.session_state.provider_config['seg'][provider]['base_url'] = st.text_input(
                f"{provider.upper()} APIåœ°å€",
                value=st.session_state.provider_config['seg'][provider]['base_url'],
                key=f"seg_{provider}_base_url"
            )
            
        with col2:
            st.subheader("å…³ç³»æå–æœåŠ¡ ")
            st.session_state.provider_config['extract']['provider'] = st.selectbox(
                "æå–æœåŠ¡å•†",
                options=["deepseek", "qwen", "doubao",'kimi'],
                index=["deepseek", "qwen", "doubao","kimi"].index(
                    st.session_state.provider_config['extract']['provider']),
                    key="extract_provider_select"
            )
            
            provider = st.session_state.provider_config['extract']['provider']
        
            
            model_options = {
                "QWQ": ["free:QwQ-32B"],
                "deepseek": ["deepseek-chat", "deepseek-reasoner"],
                "qwen": ["qwen-max", "qwen-plus", "qwen-turbo"],
                "doubao": ["doubao-1-5-pro-32k-250115", "doubao-seed-1-6-250615",'doubao-1-5-thinking-pro-250415'],
                "kimi": ["moonshot-v1-128k", "moonshot-v1-32k"]
            }
            current_models = model_options.get(provider, ["default-model"])
            current_model = st.session_state.provider_config['extract'][provider].get('model', current_models[0])
    
            if current_model not in current_models:
                current_model = current_models[0]
            st.session_state.provider_config['extract'][provider]['model'] = st.selectbox(
                f"{provider.upper()} æ¨¡å‹",
                options=current_models,
                index=current_models.index(current_model),
                key=f"extract_{provider}_model"
            )

            
            st.session_state.provider_config['extract'][provider]['api_key'] = st.text_input(
                f"{provider.upper()} APIå¯†é’¥",
                value=st.session_state.provider_config['extract'][provider]['api_key'],
                type="password",
                key=f"extract_{provider}_api_key"
            )
            
            st.session_state.provider_config['extract'][provider]['base_url'] = st.text_input(
                f"{provider.upper()} APIåœ°å€",
                value=st.session_state.provider_config['extract'][provider]['base_url'],
                key=f"extract_{provider}_base_url"
            )
        
        st.divider()
        st.subheader("æ™ºèƒ½åŠ©æ‰‹æœåŠ¡")
        seg_provider = st.session_state.provider_config['seg']['provider']
        extract_provider = st.session_state.provider_config['extract']['provider']
        available_providers = list({seg_provider, extract_provider})
        
        assistant_provider = st.selectbox(
            "é€‰æ‹©æœåŠ¡å•†ï¼ˆä»å·²é…ç½®æœåŠ¡ä¸­é€‰æ‹©ï¼‰",
            options=available_providers,
            index=0
        )
        st.session_state.assistant_provider = assistant_provider

        st.subheader("ä½¿ç”¨è¯´æ˜")
        st.markdown("""
        - æœ¬ç«™æä¾›å„ç§å¤§æ¨¡å‹apiæ¥å£ï¼Œç”¨æˆ·å¯ä»¥è‡ªè¡Œé€‰æ‹©é€‚åˆä»»åŠ¡è¦æ±‚çš„å¤§æ¨¡å‹è°ƒç”¨
        """)
        
    elif st.session_state.current_tab == 'extract':
        # ========== æ–‡æœ¬æå–é¡µé¢å†…å®¹ ==========
        st.header("åœ°è´¨æ–‡æœ¬ä¸‰å…ƒç»„æå–")
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ä¸Šä¼ DOCXæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰")
            docx_file = st.file_uploader("ä¸Šä¼ åœ°è´¨èµ„æ–™æ–‡ä»¶", type=["docx"])
            st.subheader("ä¸Šä¼ åœ°è´¨æ–‡æœ¬TXTæ–‡ä»¶")
            text_file = st.file_uploader("ä¸Šä¼ å¾…æå–çš„åœ°è´¨å·¥ç¨‹æ–‡ä»¶", type=["txt"])
            text_content = st.text_area("æˆ–ç›´æ¥è¾“å…¥æ–‡æœ¬å†…å®¹", height=300)
            
            st.subheader("å…³ç³»è§„åˆ™ (å¯é€‰)")
            rules_file = st.file_uploader("ä¸Šä¼ å…³ç³»è§„åˆ™CSV", type=["csv"])
        
        with col2:
            st.subheader("æå–è®¾ç½®")
            st.caption("APIé…ç½®è¯·åœ¨é…ç½®é¡µé¢è®¾ç½®")

            seg_provider = st.session_state.provider_config['seg']['provider']
            extract_provider = st.session_state.provider_config['extract']['provider']
        
            seg_configured = bool(st.session_state.provider_config['seg'][seg_provider]['api_key'])
            extract_configured = bool(st.session_state.provider_config['extract'][extract_provider]['api_key'])
            api_ready = seg_configured and extract_configured

            input_ready = text_content.strip() or text_file is not None
        
            button_disabled = not api_ready or not input_ready
            if not api_ready:
                st.warning("è¯·å…ˆåœ¨âš™ï¸é…ç½®é¡µé¢å®ŒæˆAPIå¯†é’¥è®¾ç½®")
            elif not input_ready:
                st.warning("è¯·ä¸Šä¼ TXTæ–‡ä»¶æˆ–è¾“å…¥æ–‡æœ¬å†…å®¹")
            
            if st.button("å¼€å§‹æå–ä¸‰å…ƒç»„", 
                    use_container_width=True,
                    disabled=button_disabled):
                
                if docx_file:  # æ–°å¢
                    with st.spinner("æ­£åœ¨å­¦ä¹ DOCXæ–‡æ¡£å†…å®¹..."):
                        extractor = RelationExtractor(
                            provider_config=st.session_state.provider_config,
                            relation_rules=rules
                        )
                        learned_content = extractor.learn_from_docx(docx_file.getvalue())
                        st.session_state.learned_content = learned_content  # å­˜å‚¨å­¦ä¹ å†…å®¹
                
                text = text_content if text_content else text_file.read().decode("utf-8")
                # å°†å­¦ä¹ å†…å®¹åˆå¹¶åˆ°è¾“å…¥æ–‡æœ¬
                if docx_file:  # æ–°å¢
                    text = f"æ–‡æ¡£çŸ¥è¯†å‚è€ƒï¼š{st.session_state.learned_content}\n\nå¾…åˆ†ææ–‡æœ¬ï¼š{text}"

                text = text_content if text_content else text_file.read().decode("utf-8")
                rules = rules_file.read().decode("utf-8") if rules_file else None
                
                extractor = RelationExtractor(
                    provider_config=st.session_state.provider_config,
                    relation_rules=rules
                )
                with st.spinner("æ­£åœ¨æå–ä¸‰å…ƒç»„ï¼Œè¯·ç¨å€™..."):
                    st.session_state.extracted_data = extractor.extract(text, rules)
                    st.session_state.current_page = 1
                    st.session_state.original_triple_count = sum(len(seg["triples"]) for seg in st.session_state.extracted_data["segments"])

                    if st.session_state.extracted_data.get("segments")and not st.session_state.extracted_data.get("has_error"):
                        st.success("ä¸‰å…ƒç»„æå–å®Œæˆï¼")
                    else:
                        st.error("ä¸‰å…ƒç»„æå–å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIé…ç½®å’Œè¾“å…¥æ–‡æœ¬")
        
        # æ˜¾ç¤ºæå–ç»“æœ
        if st.session_state.extracted_data:
            st.subheader("æå–ç»“æœæ¦‚è§ˆ")
            st.json(st.session_state.extracted_data["metadata"], expanded=False)
            
            total_segments = st.session_state.extracted_data["total_segments"]
            total_triples = sum(len(seg["triples"]) for seg in st.session_state.extracted_data["segments"])
            
            st.metric("æ€»æ®µè½æ•°", total_segments)
            st.metric("æ€»ä¸‰å…ƒç»„æ•°", total_triples)
            
            if total_segments > 0:
                seg = st.session_state.extracted_data["segments"][0]
                st.subheader(f"æ®µè½ç¤ºä¾‹: {seg['segment_id']}")
                st.caption(f"æ–‡æœ¬å†…å®¹: {seg['text_content'][:200]}...")
                
                if seg["triples"]:
                    st.write("æå–çš„ä¸‰å…ƒç»„:")
                    for triple in seg["triples"]:
                        st.code(f"{triple['subject']} ({triple['subject_type']}) â†’ {triple['predicate']} â†’ {triple['object']} ({triple['object_type']})")
                else:
                    st.info("è¯¥æ®µè½æœªæå–åˆ°ä¸‰å…ƒç»„")

    elif st.session_state.current_tab == 'correct':
        # ========== ä¸‰å…ƒç»„æ ¡æ­£é¡µé¢å†…å®¹ ==========

        if not st.session_state.extracted_data:
            st.warning("è¯·å…ˆåœ¨'æ–‡æœ¬æå–'é¡µé¢æå–ä¸‰å…ƒç»„")
        else:
            st.header("ä¸‰å…ƒç»„æ ¡æ­£")
            if not st.session_state.extracted_data:
                with st.spinner('ğŸŒ€ æ­£åœ¨åŠ è½½æ•°æ®...'):
                    time.sleep(0.5)  # æ¨¡æ‹ŸåŠ è½½è¿‡ç¨‹
                st.info("è¯·å…ˆåœ¨'æ–‡æœ¬æå–'æ ‡ç­¾é¡µæå–ä¸‰å…ƒç»„")
                return
            
  
            data = st.session_state.extracted_data
            text_keys = [seg["segment_id"] for seg in data["segments"]]
            
            col_sidebar, col_main = st.columns([1, 3])
            
            with col_sidebar:
                st.subheader("æ®µè½é€‰æ‹©")
                selected_segment_id = st.selectbox("é€‰æ‹©æ®µè½", text_keys)
                selected_segment = next(seg for seg in data["segments"] if seg["segment_id"] == selected_segment_id)
                
                st.subheader("æ–‡æœ¬å†…å®¹")
                st.caption(selected_segment["text_content"][:500] + ("..." if len(selected_segment["text_content"]) > 500 else ""))
                
                # æ–°å¢ä¸‰å…ƒç»„
                if st.button("â• æ–°å¢ä¸‰å…ƒç»„", use_container_width=True):
                    if "triples" not in selected_segment:
                        selected_segment["triples"] = []
                    
                    new_triple = {
                        "subject": "[æ–°ä¸»ä½“]",
                        "subject_type": PRESET_TYPES[0],
                        "predicate": PRESET_PREDICATES[0],
                        "object": "[æ–°å®¢ä½“]",
                        "object_type": PRESET_TYPES[0],
                        "is_custom" : True
                    }

                    selected_segment["triples"].append(new_triple)
                    st.session_state.extracted_data = data
                    # è®°å½•æ–°å¢æ“ä½œ
                    st.session_state.correction_records.append({
                        "action": "add",
                        "added": new_triple,
                        "segment_id": selected_segment_id
                    })
                    st.rerun()
                    
                # ä¸‹è½½å½“å‰çŠ¶æ€
                st.download_button(
                    label="ğŸ’¾ ä¸‹è½½å½“å‰æ•°æ®",
                    data=json.dumps(st.session_state.extracted_data, ensure_ascii=False, indent=2),
                    file_name="geological_triples.json",
                    mime="application/json"
                )
            
            with col_main:
                spo_list = selected_segment.get("triples", [])
                num_spo = len(spo_list)
                
                if num_spo == 0:
                    st.info("è¯¥æ®µè½æ²¡æœ‰ä¸‰å…ƒç»„")
                    return
                
                current_model = st.session_state.provider_config['extract'][
                    st.session_state.provider_config['extract']['provider']
                ]['model']
                st.caption(f"å½“å‰ä½¿ç”¨æ¨¡å‹ï¼š{current_model} | é…ç½®äºâš™ï¸é¡µé¢")

                # åˆ†é¡µæ§åˆ¶
                page = st.session_state.current_page - 1
                if page >= num_spo:
                    page = num_spo - 1
                    st.session_state.current_page = num_spo
                
                # åˆ†é¡µå¯¼èˆª
                col_nav1, col_nav2, col_nav3 = st.columns([1, 2, 1])
                with col_nav1:
                    if st.button("â¬…ï¸ ä¸Šä¸€ä¸ª") and page > 0:
                        st.session_state.current_page -= 1
                        st.rerun()
                with col_nav2:
                    st.markdown(f"**ä¸‰å…ƒç»„ {page+1}/{num_spo}**", help="ä½¿ç”¨å·¦å³ç®­å¤´å¯¼èˆª")
                with col_nav3:
                    if st.button("â¡ï¸ ä¸‹ä¸€ä¸ª") and page < num_spo - 1:
                        st.session_state.current_page += 1
                        st.rerun()
                
                # åˆ é™¤å½“å‰ä¸‰å…ƒç»„
                if st.button("ğŸ—‘ï¸ åˆ é™¤å½“å‰ä¸‰å…ƒç»„", type="primary"):
                    original_spo = spo_list[page].copy()
                    del spo_list[page]

                    is_original = not original_spo.get("is_custom", False)
                    st.session_state.extracted_data = data
                    # è®°å½•åˆ é™¤æ“ä½œ
                    st.session_state.correction_records.append({
                        "action": "delete",
                        "original": original_spo,
                        "segment_id": selected_segment_id,
                        "is_original": is_original
                    })
                    st.success("åˆ é™¤æˆåŠŸï¼")
                    if page >= len(spo_list) and len(spo_list) > 0:
                        st.session_state.current_page = len(spo_list)
                    st.rerun()
                
                # ç¼–è¾‘è¡¨å•
                spo = spo_list[page]
                with st.form(key="spo_form"):
                    cols = st.columns(2)
                    with cols[0]:
                        new_subject = st.text_input(
                            "ä¸»ä½“(Subject)", 
                            value=spo.get("subject", ""),
                            help="åœ°è´¨å®ä½“åç§°ï¼Œå¦‚'ç ‚å²©å±‚'"
                        )
                        new_predicate = st.selectbox(
                            "è°“è¯(Predicate)", 
                            PRESET_PREDICATES,
                            index=PRESET_PREDICATES.index(spo.get("predicate", PRESET_PREDICATES[0])),
                            help="é€‰æ‹©æˆ–è¾“å…¥åœ°è´¨å…³ç³»ç±»å‹"
                        )
                    with cols[1]:
                        new_object = st.text_input("å®¢ä½“(Object)", value=spo.get("object", ""))
                        new_subject_type = st.selectbox(
                            "ä¸»ä½“ç±»å‹", 
                            PRESET_TYPES,
                            index=PRESET_TYPES.index(spo.get("subject_type", PRESET_TYPES[0]))
                        )
                        new_object_type = st.selectbox(
                            "å®¢ä½“ç±»å‹", 
                            PRESET_TYPES,
                            index=PRESET_TYPES.index(spo.get("object_type", PRESET_TYPES[0]))
                        )
                    
                    if st.form_submit_button("ğŸ’¾ ä¿å­˜ä¿®æ”¹"):
                        original_spo = spo.copy()
                        spo_list[page] = {
                            "subject": new_subject,
                            "subject_type": new_subject_type,
                            "predicate": new_predicate,
                            "object": new_object,
                            "object_type": new_object_type,
                            "is_custom" : spo.get("is_custom", False)
                        }
                        is_original = not original_spo.get("is_custom", False)
            
                        st.session_state.extracted_data = data
                        st.session_state.correction_records.append({
                            "action": "edit",
                            "original": original_spo,
                            "modified": spo_list[page],
                            "segment_id": selected_segment_id,
                            "is_original": is_original
                        })
                        st.success("ä¿®æ”¹å·²ä¿å­˜ï¼")
                
                # å½“å‰ä¸‰å…ƒç»„é¢„è§ˆ
                st.subheader("å½“å‰ä¸‰å…ƒç»„")
                st.json(spo)

            ##å½“å‰çš„ä¸‰å…ƒç»„æ€»æ•°
            edited_count = sum(1 for r in st.session_state.correction_records if r["action"] == "edit")
            deleted_count = sum(1 for r in st.session_state.correction_records if r["action"] == "delete")
            added_count = sum(1 for r in st.session_state.correction_records if r["action"] == "add")

        

            if st.session_state.correction_records:
                st.write("è¯¦ç»†æ ¡æ­£è®°å½•:")
                for record in st.session_state.correction_records:
                    if record["action"] == "delete":
                        st.write(f"åˆ é™¤ä¸‰å…ƒç»„: {record['original']} (æ®µè½ID: {record['segment_id']})")
                    elif record["action"] == "edit":
                        st.write(f"ä¿®æ”¹ä¸‰å…ƒç»„: åŸå§‹ {record['original']} â†’ ä¿®æ”¹å {record['modified']} (æ®µè½ID: {record['segment_id']})")
                    elif record["action"] == "add":
                        st.write(f"æ–°å¢ä¸‰å…ƒç»„: {record['added']} (æ®µè½ID: {record['segment_id']})")

                    # æ˜¾ç¤ºæ ¡æ­£è®°å½•å’Œæ­£ç¡®ç‡
            st.divider()
            st.subheader("æ ¡æ­£è®°å½•ç»Ÿè®¡")
            cols = st.columns(3)
            with cols[0]:
                st.metric("ä¿®æ”¹ä¸ªæ•°", edited_count)
            with cols[1]:
                st.metric("åˆ é™¤ä¸ªæ•°", deleted_count)
            with cols[2]:
                st.metric("æ–°å¢ä¸ªæ•°", added_count)
            
    elif st.session_state.current_tab == 'export':
        # ========== å¯¼å‡ºç»“æœé¡µé¢å†…å®¹ ==========
        if not st.session_state.extracted_data:
            
            st.warning("è¯·å…ˆåœ¨'æ–‡æœ¬æå–'é¡µé¢æå–ä¸‰å…ƒç»„")
        else:
            st.header("å¯¼å‡ºç»“æœ")
            if not st.session_state.extracted_data:
                st.info("è¯·å…ˆåœ¨'æ–‡æœ¬æå–'æ ‡ç­¾é¡µæå–ä¸‰å…ƒç»„")
                return
            
            data = st.session_state.extracted_data
            
            st.subheader("JSONå¯¼å‡º")
            st.download_button(
                label="ä¸‹è½½JSONæ•°æ®",
                data=json.dumps(data, ensure_ascii=False, indent=2),
                file_name="geological_triples.json",
                mime="application/json"
            )
            
            st.subheader("CSVå¯¼å‡º")
            if st.button("ç”ŸæˆCSVæ–‡ä»¶", use_container_width=True):
                extractor = RelationExtractor(provider_config=st.session_state.provider_config)
                with st.spinner("æ­£åœ¨ç”ŸæˆCSVæ–‡ä»¶..."):
                    entities_path, relations_path = extractor.to_csv(data)
                    st.success("CSVæ–‡ä»¶ç”Ÿæˆå®Œæˆï¼")
                    
                    col_csv1, col_csv2 = st.columns(2)
                    with col_csv1:
                        # ä¿®å¤ç¼–ç é—®é¢˜ï¼šä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼è¯»å–å¹¶æŒ‡å®šUTF-8ç¼–ç 
                        with open(entities_path, "rb") as f:  # æ”¹ä¸ºäºŒè¿›åˆ¶æ¨¡å¼
                            st.download_button(
                                label="ä¸‹è½½å®ä½“è¡¨",
                                data=f,
                                file_name="entities.csv",
                                mime="text/csv",
                                key="entities_download"
                            )
                    
                    with col_csv2:
                        # ä¿®å¤ç¼–ç é—®é¢˜ï¼šä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼è¯»å–å¹¶æŒ‡å®šUTF-8ç¼–ç 
                        with open(relations_path, "rb") as f:  # æ”¹ä¸ºäºŒè¿›åˆ¶æ¨¡å¼
                            st.download_button(
                                label="ä¸‹è½½å…³ç³»è¡¨",
                                data=f,
                                file_name="relations.csv",
                                mime="text/csv",
                                key="relations_download"
                            )
            
            st.subheader("Neo4jå¯¼å…¥")
            st.code("""
            //Â ğŸ’¥Â åˆ é™¤æ‰€æœ‰æ—§æ•°æ®ï¼ˆå¯é€‰ï¼‰

            MATCHÂ (n)Â DETACHÂ DELETEÂ n;
            //Â ğŸ“ŒÂ å¯¼å…¥èŠ‚ç‚¹æ•°æ®

            LOADÂ CSVÂ WITHÂ HEADERSÂ FROMÂ 'file:///Nodes.csv'Â ASÂ row
            CREATEÂ (n:EntityÂ {
            Â Â id:Â toInteger(row.id),
            Â Â name:Â row.name,
            Â Â entity_type:Â row.entity_type
            });

            

            //Â æ­£ç¡®åˆ›å»ºåŠ¨æ€ç±»å‹çš„å…³ç³»
            LOADÂ CSVÂ WITHÂ HEADERSÂ FROMÂ 'file:///relationship.csv'Â ASÂ row
            MATCHÂ (a:EntityÂ {id:Â toInteger(row.start_id)})
            MATCHÂ (b:EntityÂ {id:Â toInteger(row.end_id)})
            CALLÂ apoc.create.relationship(a,Â row.type,Â {},Â b)Â YIELDÂ rel
            RETURNÂ count(rel);

            MATCHÂ (n)Â RETURNÂ nÂ LIMITÂ 25

            MATCHÂ (n)-[r]->(m)
            RETURNÂ n,Â r,Â m
            LIMITÂ 100;


            MATCHÂ (n:Entity)
            REMOVEÂ n:`è¯•éªŒÂ EP`,Â n:`å‹˜æµ‹æŠ€æœ¯Â RH`,Â n:`åœ°è´¨å·¥ç¨‹Â PC`,Â n:`åœ°è´¨èµ„æ–™Â GI`,Â n:`åœ°è´¨ç‰¹å¾Â GF`;


            MATCHÂ (n:Entity)
            WITHÂ n.entity_typeÂ ASÂ et,Â collect(n)Â ASÂ batch
            CALLÂ apoc.create.addLabels(batch,Â [et])Â YIELDÂ node
            RETURNÂ count(node)Â ASÂ æ ‡è®°èŠ‚ç‚¹æ€»æ•°;

            :style
            node.EntityÂ {
            Â Â size:Â 40px;
            Â Â caption:Â '{name}';
            Â Â color:Â grey;
            }
            node.`è¯•éªŒÂ EP`Â {
            Â Â color:Â #e67e22;Â //Â æ©™è‰²
            }
            node.`å‹˜æµ‹æŠ€æœ¯Â RH`Â {
            Â Â color:Â #9b59b6;Â //Â ç´«è‰²
            }
            node.`åœ°è´¨å·¥ç¨‹Â PC`Â {
            Â Â color:Â #3498db;Â //Â è“è‰²
            }
            node.`åœ°è´¨èµ„æ–™Â GI`Â {
            Â Â color:Â #f1c40f;Â //Â é»„è‰²
            }
            node.`åœ°è´¨ç‰¹å¾Â GF`Â {
            Â Â color:Â #2ecc71;Â //Â ç»¿è‰²
            }
            relationshipÂ {
            Â Â color:Â #e74c3c;
            Â Â caption:Â '{type}';
            }
                    """)
    elif st.session_state.current_tab == 'assistant':
        st.header("æ™ºèƒ½æç¤ºè¯åŠ©æ‰‹")
        if st.session_state.extracted_data:
            with st.expander("ğŸ“Œ å½“å‰æå–å†…å®¹å‚è€ƒ", expanded=True):
                st.caption("ä»¥ä¸‹ä¸ºæœ€æ–°æå–å†…å®¹ï¼Œå¯ç”¨äºæç¤ºè¯ä¼˜åŒ–å‚è€ƒ")
                total_segments = st.session_state.extracted_data["total_segments"]
                total_triples = sum(len(seg["triples"]) for seg in st.session_state.extracted_data["segments"])
                
                cols = st.columns([1,2])
                with cols[0]:
                    st.metric("æ€»æ®µè½æ•°", total_segments)
                    st.metric("æ€»ä¸‰å…ƒç»„æ•°", total_triples)
                    
                with cols[1]:
                    sample_segment = st.session_state.extracted_data["segments"][0]
                    st.caption("ç¤ºä¾‹æ®µè½å†…å®¹ï¼ˆå‰200å­—ï¼‰:")
                    st.code(sample_segment["text_content"][:200] + "...")
                    
                    # ç§»é™¤åµŒå¥—çš„expanderï¼Œæ”¹ä¸ºæ™®é€šæ˜¾ç¤º
                    st.caption("å®Œæ•´æ®µè½å†…å®¹ï¼ˆå‰500å­—ï¼‰:")
                    st.text(sample_segment["text_content"][:500] + ("..." if len(sample_segment["text_content"]) > 500 else ""))
                    
                    if sample_segment["triples"]:
                        st.caption("ç¤ºä¾‹ä¸‰å…ƒç»„:")
                        st.json(sample_segment["triples"][0])

        if not hasattr(st.session_state, 'assistant_provider') or not st.session_state.assistant_provider:
            st.error("è¯·å…ˆåœ¨é…ç½®é¡µé¢å®Œæˆåˆ†å¥å’Œå…³ç³»æå–æœåŠ¡çš„é…ç½®")
            return
        provider = st.session_state.assistant_provider
        try:
            provider = st.session_state.assistant_provider
            config = None

            # åˆ¤æ–­æœåŠ¡å•†å±äºåˆ†å¥è¿˜æ˜¯å…³ç³»æå–æœåŠ¡
            if provider in st.session_state.provider_config['seg']:
                config = st.session_state.provider_config['seg'][provider]
            elif provider in st.session_state.provider_config['extract']:
                config = st.session_state.provider_config['extract'][provider]
                
            if not config or not config.get('api_key'):
                st.error(f"ã€å…³é”®ä¿®å¤ã€‘{provider} APIå¯†é’¥æœªæ­£ç¡®é…ç½®ï¼Œè¯·ç¡®è®¤ï¼š")
                st.error("1. å·²åœ¨é…ç½®é¡µé¢ä¿å­˜è¿‡è¯¥æœåŠ¡å•†çš„é…ç½®")
                st.error("2. æ¨¡å‹é€‰æ‹©ä¸APIå¯†é’¥åŒ¹é…")
                return
        except KeyError as e:
            st.error(f"æœåŠ¡å•†é…ç½®é”™è¯¯: {str(e)}ï¼Œè¯·æ£€æŸ¥é…ç½®é¡µé¢")
            return
        
        if "prompt_history" not in st.session_state:
            st.session_state.prompt_history = []
        # åˆå§‹åŒ–å¯¹è¯å†å²
        if "messages" not in st.session_state:
            st.session_state.messages = []
        if st.session_state.prompt_history:
            with st.expander("ğŸ•’ æç¤ºè¯ä¿®æ”¹å†å²", expanded=True):
                for idx, record in enumerate(st.session_state.prompt_history):
                    cols = st.columns([1,3,2])
                    with cols[0]:
                        st.markdown(f"**ç‰ˆæœ¬ {idx+1}**")
                    with cols[1]:
                        st.caption(f"ä¿®æ”¹æ—¶é—´: {datetime.fromisoformat(record['modified'].get('generated_at', datetime.now().isoformat()))}")
                    with cols[2]:
                        if st.button("â®ï¸ å›æ»šåˆ°æ­¤ç‰ˆæœ¬", key=f"revert_{idx}"):
                            # æ¢å¤å†å²ç‰ˆæœ¬
                            PRESET_PROMPTS.clear()
                            PRESET_PROMPTS.update(record["original"])
                            # ç§»é™¤ä¹‹åçš„ä¿®æ”¹è®°å½•
                            st.session_state.prompt_history = st.session_state.prompt_history[:idx]
                            st.session_state.extracted_data = None
                            st.success("å·²å›æ»šåˆ°æ­¤ç‰ˆæœ¬ï¼Œè¯·é‡æ–°æ‰§è¡Œæå–æ“ä½œ")
                            st.rerun()
        
        
        # æ˜¾ç¤ºå†å²å¯¹è¯
        for msg in st.session_state.messages:
            st.chat_message(msg["role"]).write(msg["content"])


        # å¯¹è¯è¾“å…¥
        if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨å¯¹æç¤ºè¯çš„æ”¹è¿›è¦æ±‚"):
            # åˆå§‹åŒ–æ™ºèƒ½åŠ©æ‰‹
            assistant = ChatOpenAI(
                api_key=config['api_key'],
                base_url=config['base_url'],
                model=config['model'],
                temperature=0.5
            )
            # æ„å»ºç³»ç»Ÿæç¤º
            system_prompt = f"""ä½œä¸ºä¸“ä¸šçš„æç¤ºè¯ä¼˜åŒ–ä¸“å®¶ï¼Œè¯·æŒ‰ä»¥ä¸‹è¦æ±‚æ”¹è¿›å…³ç³»æå–æç¤ºè¯ï¼š
    
            ### æ”¹è¿›ç›®æ ‡
            1. ä¿æŒæ ¸å¿ƒè¦ç´ ï¼šå®ä½“ç±»å‹({PRESET_TYPES})ã€å…³ç³»ç±»å‹({PRESET_PREDICATES})
            2. ä¼˜åŒ–æŒ‡ä»¤æ¸…æ™°åº¦ï¼Œå‡å°‘æ­§ä¹‰
            3. å¢å¼ºé¢†åŸŸä¸“ä¸šæ€§ï¼ˆåœ°è´¨å·¥ç¨‹ï¼‰
            4. ä¿æŒå˜é‡å ä½ç¬¦ï¼š{{text}}, {{entity_types}}, {{predicate_list}}

            ### æ ¼å¼è§„èŒƒ
            #### å…³ç³»æå–
            [åœ¨æ­¤ç¼–å†™æ”¹è¿›åçš„å®Œæ•´æç¤ºè¯]
            - ä½¿ç”¨ä¸­æ–‡æ ‡ç‚¹
            - é¿å…Markdownæ ¼å¼
            - åŒ…å«å®Œæ•´ç¤ºä¾‹

            ### ä¿®æ”¹ç¤ºä¾‹
            åŸå¥ï¼šè¯·æå–ç›¸å…³å…³ç³»
            æ”¹ä¸ºï¼šè¯·æ ¹æ®åœ°è´¨å·¥ç¨‹è§„èŒƒï¼Œè¯†åˆ«[ä¸»ä½“ç±»å‹]ä¸[å®¢ä½“ç±»å‹]ä¹‹é—´çš„{PRESET_PREDICATES}å…³ç³»

            å½“å‰æ¨¡æ¿ï¼š
            {json.dumps(PRESET_PROMPTS['å…³ç³»æå–'], indent=2, ensure_ascii=False)}
            """
            
            # æ·»åŠ è¾“å…¥é¢„å¤„ç†
            cleaned_prompt = re.sub(r'[æ¨¡ç³Š|å¤§æ¦‚|å¯èƒ½]', '', prompt)  # å»é™¤æ¨¡ç³Šè¡¨è¿°
            if len(cleaned_prompt) < 10:
                st.warning("è¯·æä¾›æ›´å…·ä½“çš„æ”¹è¿›éœ€æ±‚ï¼Œä¾‹å¦‚ï¼š'éœ€è¦å¢åŠ å®ä½“ç±»å‹ç¤ºä¾‹'")
                return

             # æ‰§è¡Œå¯¹è¯
            st.session_state.messages.append({"role": "user", "content": cleaned_prompt})
            with st.chat_message("assistant"):
                try:
                    response = assistant.invoke([
                        SystemMessage(content=system_prompt),
                        HumanMessage(content=cleaned_prompt)
                    ])
                    
                    # æ·»åŠ åå¤„ç†éªŒè¯
                    if not re.search(r'\{entity_types\}.*?\{predicate_list\}', response.content):
                        raise ValueError("æç¤ºè¯ç¼ºå°‘å¿…è¦å˜é‡")
                        
                    st.markdown(response.content)
                    
                    # æ·»åŠ æ ¼å¼ç¾åŒ–
                    st.markdown("---")
                    with st.expander("âœ… éªŒè¯é€šè¿‡"):
                                st.caption("åŒ…å«å¿…è¦è¦ç´ ï¼š")
                                cols = st.columns(3)
                                cols[0].success("å®ä½“ç±»å‹")
                                cols[1].success("å…³ç³»ç±»å‹") 
                                cols[2].success("å˜é‡å ä½ç¬¦")
            
                    col_accept, col_reject = st.columns(2)
                    with col_accept:
                        if st.button("âœ… æ¥å—å»ºè®®", key="accept_btn"):
                            updated_prompts = parse_prompt_update(response.content)
                            if updated_prompts:
                                PRESET_PROMPTS.update(updated_prompts)
                                st.session_state.prompt_history.append({
                                    "original": PRESET_PROMPTS.copy(),
                                    "modified": updated_prompts
                                })
                                st.session_state.extracted_data = None
                                st.rerun()
                    
                    with col_reject:
                        if st.button("âŒ æ‹’ç»å»ºè®®", key="reject_btn"):
                            st.session_state.messages.pop()
                            st.rerun()

                except Exception as e:
                    st.error(f"ä¼˜åŒ–å¤±è´¥ï¼š{str(e)}")
                    st.info("è¯·å°è¯•æ›´æ˜ç¡®çš„ä¿®æ”¹è¦æ±‚ï¼Œä¾‹å¦‚ï¼š'éœ€è¦æ›´ä¸¥æ ¼çš„å…³ç³»ç±»å‹è¿‡æ»¤'")



if __name__ == "__main__":
    main()
